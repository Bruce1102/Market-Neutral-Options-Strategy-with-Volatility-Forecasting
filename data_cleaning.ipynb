{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing Necessary Libraries and Modules </h3>\n",
    "In this section, we'll import the necessary libraries and modules required for our data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from src.data_handling import load_csv_data, closest_date, save_to_csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading Data </h3> \n",
    "\n",
    "We'll load the data for different titles into a list of DataFrames. Each title corresponds to a CSV file that we'll read and preprocess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path and titles\n",
    "PATH = \"DATA/raw\"\n",
    "DF_TITLES = [\"AAPL\", \"AMZN\", \"MSFT\", \"SPX\", \"VIX\", \"IRX\"]\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_data(title: str) -> pd.DataFrame:\n",
    "    temp_df = pd.read_csv(f\"{PATH}/{title}.csv\")\n",
    "    \n",
    "    # Create new column names\n",
    "    new_columns = [\"Date\"] + [f\"{title}_{col}\" for col in temp_df.columns if col != \"Date\"]\n",
    "    \n",
    "    # Check if the number of new column names matches the number of columns in the DataFrame\n",
    "    if len(new_columns) != len(temp_df.columns):\n",
    "        raise ValueError(f\"Column name mismatch for {title}.csv\")\n",
    "    \n",
    "    temp_df.columns = new_columns\n",
    "    temp_df[\"Date\"] = pd.DatetimeIndex(temp_df[\"Date\"])\n",
    "    return temp_df.set_index(\"Date\")\n",
    "\n",
    "\n",
    "# Load data into a list of DataFrames\n",
    "df_list = [load_data(title) for title in DF_TITLES]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data Preprocessing </h3>\n",
    "\n",
    "In this section, we'll:\n",
    "1. Filter the data based on the latest start date.\n",
    "2. Handle exceptions for specific titles.\n",
    "3. Concatenate the data into a single DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest start date\n",
    "latest_start = max([df.index[0] for df in df_list])\n",
    "earliest_end = min(([df.index[-1] for df in df_list]))\n",
    "# Apply date filter\n",
    "df_list = [df[df.index >= latest_start] for df in df_list]\n",
    "df_list = [df[df.index <= earliest_end] for df in df_list]\n",
    "\n",
    "# Handle exception for VIX\n",
    "date_set = set(df_list[0].index)\n",
    "df_list[4] = df_list[4][df_list[4].index.isin(date_set)]\n",
    "\n",
    "# Concatenate to a single DataFrame and drop unnecessary columns\n",
    "df = pd.concat(df_list, axis=1)\n",
    "df = df.drop([\"VIX_Volume\", \"IRX_Volume\"], axis=1)\n",
    "\n",
    "# Filtering weekends\n",
    "df = df[[date.isoweekday() <= 5 for date in df.index]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Handling Missing Dates</h3>\n",
    "\n",
    "To ensure continuity in our time series data, we'll:\n",
    "1. Identify missing dates.\n",
    "2. Fill the missing dates with average values from neighboring dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all dates and the range\n",
    "all_dates = set(df.index)\n",
    "start_date, end_date = df.index[0], df.index[-1]\n",
    "date_range = pd.date_range(start_date, end_date)\n",
    "\n",
    "# Identify missing dates\n",
    "missing_dates = date_range.difference(df.index)\n",
    "missing_dates = missing_dates[missing_dates.to_series().apply(lambda x: x.isoweekday() <= 5)]\n",
    "\n",
    "# Adding NaN's to missing dates\n",
    "nan_rows = df[df.isna().any(axis=1)].index\n",
    "missing_dates = missing_dates.append(nan_rows)\n",
    "\n",
    "# Fill missing dates with average values\n",
    "for date in missing_dates:\n",
    "    closest_up, closest_down = closest_date(date, df)\n",
    "    avg_data = df.loc[[closest_up, closest_down]].mean()\n",
    "    df.loc[date] = avg_data\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df = df.sort_index()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving the Processed Data</h3>\n",
    "\n",
    "Finally, we'll save the processed data to a CSV file for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to DATA/processed/df.csv\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"DATA/processed/df.csv\"\n",
    "\n",
    "save_to_csv(df, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4818a0b8c316263be072c2082609790d2bac6bbfe2378382b84905edb944ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
